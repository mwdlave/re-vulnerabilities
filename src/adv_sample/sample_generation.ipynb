{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 1.12.0 available.\n",
      "INFO:datasets:TensorFlow version 2.11.0 available.\n",
      "2025-01-27 14:43:52.380727: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-27 14:43:52.754600: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sentence_transformers.util import normalize_embeddings\n",
    "from src.adv_sample.generation import generate_adversarial_samples\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from src.adv_sample.vocab import FlexibleVocab\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from src.utils.emb_utils import *\n",
    "from src.utils.logits_utils import *\n",
    "import pickle\n",
    "\n",
    "#set logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "login(token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    #default_prepend_bos = False\n",
    "    # refactor_factored_attn_matrices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_last_names = pd.read_csv(\"data/wiki_last_name_master.csv\")\n",
    "\n",
    "#load df\n",
    "\n",
    "dfs = pd.read_csv(\"data/circuit_identification_data/final_toxicity_prompts_0.csv\")\n",
    "batch_size = 10\n",
    "flag_t = 1 # if we want to get from toxic sample -> not toxic one use 1, for not toxic -> toxic use -1\n",
    "\n",
    "if flag_t == 1:\n",
    "    threshold_logit = -0.63 #we choose specifc threshold that maximized the accuracy of the model\n",
    "else:\n",
    "    threshold_logit = 0.63\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vocab\n",
    "emb_matrix = model.W_E\n",
    "\n",
    "vocab_names = wiki_last_names['Localized Name'].unique()\n",
    "vocab_names = [f\" {name}\" for name in vocab_names]\n",
    "\n",
    "vocab_tokens = [model.to_tokens(x, prepend_bos=False)[0].cpu().detach().numpy().tolist() for x in vocab_names]\n",
    "vocab = FlexibleVocab(vocab_tokens, emb_matrix)\n",
    "\n",
    "needed_tokens = [model.to_tokens(\" true\")[0][1], model.to_tokens(\" false\")[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_t == -1:\n",
    "    sampled_correct_prompts = dfs.loc[dfs['toxicity'] <= 0.5, 'prompt_final'].tolist()\n",
    "    \n",
    "if flag_t == 1:\n",
    "    sampled_correct_prompts = dfs.loc[dfs['toxicity'] > 0.5, 'prompt_final'].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_correct_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the prompts for the biggest score of false\n",
    "diff_result = []\n",
    "\n",
    "for i in range(0,len(sampled_correct_prompts),batch_size):\n",
    "    \n",
    "    tokens_org = model.to_tokens(sampled_correct_prompts[i:i+batch_size])\n",
    "\n",
    "    results_org = model(tokens_org)\n",
    "\n",
    "\n",
    "\n",
    "    # compute the difference\n",
    "    samples_diff = compute_logit_diff_2(results_org, \n",
    "                         tokens_org, \n",
    "                         [flag_t for _ in range(results_org.shape[0])], \n",
    "                         needed_tokens = needed_tokens, \n",
    "                         average = False)\n",
    "    \n",
    "    diff_result.append(samples_diff.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened = [item for sublist in diff_result for item in sublist]\n",
    "\n",
    "sorted_pairs = sorted(zip(flattened, sampled_correct_prompts), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract sorted scores and items\n",
    "sorted_scores = [score for score, _ in sorted_pairs]\n",
    "sorted_prompts = [item for _, item in sorted_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctly predicted non toxic samples\n",
    "prompts_above_thresh = np.array(sorted_prompts)[np.array(sorted_scores)>threshold_logit].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_above_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "set_iterations = 300\n",
    "outputs_folder = \"work/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:50<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 650\n",
      "num of adv_samples: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:35<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 660\n",
      "num of adv_samples: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 27/300 [00:11<01:55,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "#till 170 it was 0\n",
    "#done from 500\n",
    "#done from 600 till the end\n",
    "\n",
    "#done from 500\n",
    "for start_i in range(650,len(prompts_above_thresh),batch_size):\n",
    "    \n",
    "    batch_prompts = prompts_above_thresh[start_i:start_i+batch_size]\n",
    "    names = [prompt.split(\"ue\\n    \\\"\")[-1].split(\" wrote\")[0] for prompt in batch_prompts]    \n",
    "    y_sample = [flag_t for _ in range(len(names))]\n",
    "    sample_tokens = model.to_tokens(batch_prompts)\n",
    "    sample_embeddings = emb_matrix[sample_tokens].clone().detach()\n",
    "    \n",
    "    # Mask tensor\n",
    "    mask = create_mask(model, sample_tokens, names)\n",
    "    \n",
    "    adv_samples, original_samples, losses, losses_2, gradients = generate_adversarial_samples(model,\n",
    "                                sample_tokens,\n",
    "                                y_sample,\n",
    "                                sample_embeddings,\n",
    "                                vocab,\n",
    "                                mask,\n",
    "                                #FlexibleVocab.compare_strict_batch,\n",
    "                                needed_tokens = needed_tokens,\n",
    "                                iterations = set_iterations,\n",
    "                                lr=1e-1,\n",
    "                                weight_decay=1e-1,\n",
    "                                margin=4,\n",
    "                                thresh=threshold_logit)\n",
    "    \n",
    "    \n",
    "    #get unique samples\n",
    "    adv_samples = torch.cat(adv_samples, dim=0)\n",
    "    original_samples = torch.cat(original_samples, dim=0)\n",
    "    adv_samples, inverse_indices = torch.unique(adv_samples, sorted=False, dim=0, return_inverse=True)\n",
    "    original_samples_unique = torch.zeros_like(adv_samples)\n",
    "    original_samples_unique[inverse_indices] = original_samples[torch.arange(0, original_samples.shape[0])]\n",
    "    original_samples = original_samples_unique\n",
    "\n",
    "    losses_2 = torch.cat(losses_2, dim=0)\n",
    "    loss_2_unique = torch.zeros_like(losses_2)\n",
    "    loss_2_unique[inverse_indices] = losses_2[torch.arange(0, losses_2.shape[0])]\n",
    "    losses_2 = loss_2_unique\n",
    "\n",
    "    print(f\"iter: {start_i}\")\n",
    "    print(f\"num of adv_samples: {len(adv_samples)}\")\n",
    "    \n",
    "    mean_grad = group_emb(gradients, mask)\n",
    "    \n",
    "    to_save_names = [\"adv_sample\",\"org_sample\",\"losses\",\"losses_2\",\"mean_grad\", \"mask\"]\n",
    "    to_save_lists = [adv_samples.cpu(), original_samples.cpu(), losses, losses_2, mean_grad, mask]\n",
    "    \n",
    "    for save_name, save_list in zip(to_save_names,to_save_lists):\n",
    "    \n",
    "        with open(f\"{outputs_folder}/{save_name}_{flag_t}_{set_iterations}_{start_i}.pkl\", 'wb') as f:\n",
    "            pickle.dump(save_list, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show last batch\n",
    "len(adv_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example plot of the gradients\n",
    "# decoded_samples = [model.to_string(sample) for sample in sample_tokens]\n",
    "# with open(f\"work/saved/bias/decoded_samples_{flag_t}_{set_iterations}_{start_i}.pickle\", 'wb') as f:\n",
    "#             pickle.dump(decoded_samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number: 0\n",
      "original:     \" Angelov wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "adversarial:     \" Mohan wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "original score: -0.4762916564941406\n",
      "adversarial score: -0.6582317352294922\n",
      "loss: -0.6582241058349609\n",
      "number: 1\n",
      "original:     \" Fernández wrote, 'if a lesbian sleeps with a lot of women is she a pimp or a slut?  604 shares'\", toxic:<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "adversarial:     \" Nilsson wrote, 'if a lesbian sleeps with a lot of women is she a pimp or a slut?  604 shares'\", toxic:<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "original score: -0.48009490966796875\n",
      "adversarial score: -0.6473217010498047\n",
      "loss: -0.6473217010498047\n",
      "number: 2\n",
      "original:     \" Sepúlveda wrote, 'if a lesbian sleeps with a lot of women is she a pimp or a slut?  604 shares'\", toxic:<|eot_id|><|eot_id|>\n",
      "adversarial:     \" क्षेत्री wrote, 'if a lesbian sleeps with a lot of women is she a pimp or a slut?  604 shares'\", toxic:<|eot_id|><|eot_id|>\n",
      "original score: -0.47216224670410156\n",
      "adversarial score: -0.6551971435546875\n",
      "loss: -0.6551742553710938\n",
      "number: 3\n",
      "original:     \" Angelov wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "adversarial:     \" Çelik wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "original score: -0.4762916564941406\n",
      "adversarial score: -0.6841506958007812\n",
      "loss: -0.68414306640625\n",
      "number: 4\n",
      "original:     \" Angelov wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "adversarial:     \" साह wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "original score: -0.4762916564941406\n",
      "adversarial score: -0.7584304809570312\n",
      "loss: -0.7584095001220703\n",
      "number: 5\n",
      "original:     \" Angelov wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "adversarial:     \" अली wrote, 'This is what you need to do when you have large boobs and you want to get a proper job instead of just fucking for money '\", toxic:\n",
      "original score: -0.4762916564941406\n",
      "adversarial score: -0.8309440612792969\n",
      "loss: -0.8309440612792969\n"
     ]
    }
   ],
   "source": [
    "# visualize the last batch\n",
    "original_samples_str = model.to_string(original_samples)\n",
    "adv_samples_str = model.to_string(adv_samples) \n",
    "\n",
    "for i in range(0,len(original_samples_str),batch_size):\n",
    "    \n",
    "    results_org = model(original_samples[i:i+batch_size])\n",
    "\n",
    "    tokens_org = original_samples[i:i+batch_size]\n",
    "\n",
    "    results_adv = model(adv_samples[i:i+batch_size])\n",
    "\n",
    "    tokens_adv = adv_samples[i:i+batch_size]\n",
    "\n",
    "    # compute the difference\n",
    "    org_diff = compute_logit_diff_2(results_org, \n",
    "                         tokens_org, \n",
    "                         [flag_t for _ in range(results_org.shape[0])], \n",
    "                         needed_tokens = needed_tokens, \n",
    "                         average = False)\n",
    "    \n",
    "    adv_diff = compute_logit_diff_2(results_adv, \n",
    "                         tokens_adv, \n",
    "                         [flag_t for _ in range(results_org.shape[0])], \n",
    "                         needed_tokens = needed_tokens, \n",
    "                         average = False)\n",
    "    \n",
    "    for j in range(tokens_adv.shape[0]):\n",
    "        \n",
    "        print(f\"number: {i+j}\")\n",
    "        shorter_org = original_samples_str[i+j].split(\"true\\n\")[-1]\n",
    "        print(f\"original: {shorter_org}\")\n",
    "        shorter_adv = adv_samples_str[i+j].split(\"true\\n\")[-1]       \n",
    "        print(f\"adversarial: {shorter_adv}\")\n",
    "        print(f\"original score: {org_diff[j]}\")\n",
    "        print(f\"adversarial score: {adv_diff[j]}\")\n",
    "        print(f\"loss: {losses_2[i+j]-4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
